## Описание работы кода
### Начало работы
```python
from pyspark import SparkContext, SparkConf
```
### Настройка Spark
```python
app_name = "Lab1"
conf = SparkConf().setAppName(app_name).setMaster('local[1]')
sc = SparkContext(conf=conf)
```
Этот участок кода создает SparkContext с именем приложения "Lab1", используя локальный режим с одним исполнителем.

### Загрузка данных
```python
!hadoop fs -put datasets data
!hadoop fs -ls data
```
Эти команды загружают данные в файловую систему Hadoop, а затем отображают содержимое директории data.

### Подготовка данных
Код включает в себя определение классов Station и Trip, а также методы initStation и initTrip, которые используются для инициализации объектов станций и поездок на основе полученных данных.

### Чтение данных
```python
trip_data = sc.textFile("data/trips.csv")
station_data = sc.textFile("data/stations.csv")
```
Этот участок кода загружает данные о поездках и станциях в Spark.

### Анализ данных
Производится анализ данных, включающий в себя различные запросы:

1. Поиск велосипеда с максимальным временем пробега: Вычисление суммарного времени поездок для каждого велосипеда и определение велосипеда с максимальным временем пробега.

2. Нахождение наибольшего геодезического расстояния между станциями: Подсчет средней продолжительности поездок между станциями, и вывод пяти самых длинных путей.

3. Поиск пути велосипеда с максимальным временем пробега через станции: Выявление маршрута поездки велосипеда с самым продолжительным временем движения через станции.

4. Определение количества велосипедов в системе: Подсчет уникальных идентификаторов велосипедов для определения их общего числа.

5. Поиск пользователей, потративших на поездки более 3 часов: Идентификация пользователей, совершивших поездки продолжительностью более 3 часов.

### Завершение работы
```python
sc.stop()
```
Этот участок кода завершает работу Spark и останавливает SparkContext.

## Как использовать
Установите Apache Spark и настройте среду выполнения PySpark.
Запустите код из Jupyter Notebook или интерпретатора Python, поддерживающего Spark.
Изучите результаты выполнения различных аналитических запросов к данным о поездках на велосипедах.
